{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy import stats\n",
    "import math\n",
    "\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable          Type         Data/Info\n",
      "----------------------------------------\n",
      "PCA               ABCMeta      <class 'sklearn.decomposition.pca.PCA'>\n",
      "StandardScaler    type         <class 'sklearn.preproces<...>ing.data.StandardScaler'>\n",
      "dt_sampleSubmit   DataFrame                id  loss\\n0  <...>[125546 rows x 2 columns]\n",
      "dt_test           DataFrame                id cat1 cat2 <...>25546 rows x 131 columns]\n",
      "dt_train          DataFrame                id cat1 cat2 <...>88318 rows x 132 columns]\n",
      "math              module       <module 'math' from '/Use<...>3.5/lib-dynload/math.so'>\n",
      "matplotlib        module       <module 'matplotlib' from<...>/matplotlib/__init__.py'>\n",
      "np                module       <module 'numpy' from '/Us<...>kages/numpy/__init__.py'>\n",
      "path_submit       str          ../data/Allstate_Claims_S<...>ity/sample_submission.csv\n",
      "path_test         str          ../data/Allstate_Claims_Severity/test.csv\n",
      "path_train        str          ../data/Allstate_Claims_Severity/train.csv\n",
      "pd                module       <module 'pandas' from '/U<...>ages/pandas/__init__.py'>\n",
      "plt               module       <module 'matplotlib.pyplo<...>es/matplotlib/pyplot.py'>\n",
      "stats             module       <module 'scipy.stats' fro<...>scipy/stats/__init__.py'>\n"
     ]
    }
   ],
   "source": [
    "%run \"0_load.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Basic Data Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for generalisation\n",
    "ID = 'id'\n",
    "TARGET = 'loss'\n",
    "\n",
    "# y\n",
    "y_train = dt_train[TARGET].ravel()\n",
    "\n",
    "# test id for submission\n",
    "id_test = dt_test[ID].ravel()\n",
    "\n",
    "# drop ID and TARGET\n",
    "dt_train.drop([ID, TARGET], axis = 1, inplace = True)\n",
    "dt_test.drop([ID], axis = 1, inplace = True)\n",
    "\n",
    "# feature names\n",
    "features = dt_train.columns\n",
    "col_cat = dt_train.select_dtypes([\"object\"]).columns.values\n",
    "col_num = dt_train.select_dtypes([\"float\"]).columns.values\n",
    "col_num = np.setdiff1d(col_num, [\"loss\"])\n",
    "\n",
    "# rows\n",
    "nrow_train = dt_train.shape[0]\n",
    "nrow_test = dt_test.shape[0]\n",
    "\n",
    "# concat\n",
    "dt_all = pd.concat((dt_train, dt_test)).reset_index(drop = True)\n",
    "\n",
    "# factorization\n",
    "# for col in col_cat:\n",
    "#     dt_all[col] = pd.factorize(dt_all[col], sort = True)[0]\n",
    "\n",
    "# grouped target\n",
    "groupedTarget = np.zeros(nrow_train, dtype = \"object\")\n",
    "for i in range(12):\n",
    "    if (i == 0):\n",
    "        myindex = (y_train < ((i*1000) + 1000) )\n",
    "    if (i > 0) & ( i < 11):\n",
    "        myindex = (y_train > ((i*1000) -1) ) & (y_train < ((i*1000) + 1000) )\n",
    "    if (i == 11):\n",
    "        myindex = (y_train > ((i*1000) -1) )\n",
    "    groupedTarget[myindex] = i\n",
    "    \n",
    "# x\n",
    "x_train = np.array(dt_all.iloc[:nrow_train, :])\n",
    "x_test = np.array(dt_all.iloc[nrow_train, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Advanced Data Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Simple Interaction on col_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simpleInter_num(dt_num, interaction = [\"+\", \"-\", \"*\"]):\n",
    "    ## +\n",
    "    if(\"+\" in interaction):\n",
    "        \n",
    "        dt_sum = pd.DataFrame()\n",
    "        \n",
    "        for i in dt_num.columns:\n",
    "            for j in dt_num.columns:\n",
    "                if(i != j \n",
    "                   and \"sum_\" + i + \"_\" + j not in dt_sum.columns \n",
    "                   and \"sum_\" + j + \"_\" + i not in dt_sum.columns):\n",
    "                    dt_sum[\"sum_\" + i + \"_\" + j] = dt_num[i] + dt_num[j]\n",
    "    \n",
    "    ## -\n",
    "    if(\"-\" in interaction):\n",
    "        \n",
    "        dt_sub = pd.DataFrame()\n",
    "        \n",
    "        for i in dt_num.columns:\n",
    "            for j in dt_num.columns:\n",
    "                if(i != j \n",
    "                   and \"sub_\" + i + \"_\" + j not in dt_sub.columns \n",
    "                   and \"sub_\" + j + \"_\" + i not in dt_sub.columns):\n",
    "                    dt_sub[\"sub_\" + i + \"_\" + j] = dt_num[i] - dt_num[j]\n",
    "                    \n",
    "    ## *\n",
    "    if(\"*\" in interaction):\n",
    "        \n",
    "        dt_times = pd.DataFrame()\n",
    "        \n",
    "        for i in dt_num.columns:\n",
    "            for j in dt_num.columns:\n",
    "                if(i != j \n",
    "                   and \"times_\" + i + \"_\" + j not in dt_times.columns \n",
    "                   and \"times_\" + j + \"_\" + i not in dt_times.columns):\n",
    "                    dt_times[\"times_\" + i + \"_\" + j] = dt_num[i] * dt_num[j]\n",
    "    \n",
    "    return(dt_sum, dt_sub, dt_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# final ouput table\n",
    "dt_all_sum, dt_all_sub, dt_all_times = simpleInter_num(dt_all.loc[:, col_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(313864, 91) (313864, 91) (313864, 91)\n"
     ]
    }
   ],
   "source": [
    "print(\"{} {} {}\".format(dt_all_sum.shape, dt_all_sub.shape, dt_all_times.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 highly correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# correlation\n",
    "correlations = dt_all.loc[:, col_num].corr()\n",
    "\n",
    "# Set the threshold to select only highly correlated attributes\n",
    "threshold = 0.5\n",
    "\n",
    "# List of pairs along with correlation above threshold\n",
    "corr_list = []\n",
    "\n",
    "#Search for the highly correlated pairs\n",
    "for i in range(0,len(col_num)): #for 'size' features\n",
    "    for j in range(i+1,len(col_num)): #avoid repetition\n",
    "        if (correlations.iloc[i,j] >= threshold and correlations.iloc[i,j] < 1) or (correlations.iloc[i,j] < 0 and correlations.iloc[i,j] <= -threshold):\n",
    "            corr_list.append([correlations.iloc[i,j],i,j]) #store correlation and columns index\n",
    "\n",
    "# Sort to show higher ones first            \n",
    "s_corr_list = sorted(corr_list,key=lambda x: -abs(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cont11 and cont12 = 0.99\n",
      "cont1 and cont9 = 0.93\n",
      "cont10 and cont6 = 0.88\n",
      "cont13 and cont6 = 0.81\n",
      "cont1 and cont10 = 0.81\n",
      "cont6 and cont9 = 0.80\n",
      "cont12 and cont6 = 0.79\n",
      "cont10 and cont9 = 0.79\n",
      "cont11 and cont6 = 0.77\n",
      "cont1 and cont6 = 0.76\n",
      "cont11 and cont7 = 0.75\n",
      "cont12 and cont7 = 0.74\n",
      "cont10 and cont12 = 0.72\n",
      "cont10 and cont13 = 0.71\n",
      "cont10 and cont11 = 0.70\n",
      "cont6 and cont7 = 0.66\n",
      "cont13 and cont9 = 0.64\n",
      "cont12 and cont9 = 0.63\n",
      "cont1 and cont12 = 0.61\n",
      "cont11 and cont9 = 0.61\n",
      "cont1 and cont11 = 0.60\n",
      "cont1 and cont13 = 0.53\n",
      "cont4 and cont8 = 0.53\n"
     ]
    }
   ],
   "source": [
    "# Print correlations and column names\n",
    "for v,i,j in s_corr_list:\n",
    "    print (\"%s and %s = %.2f\" % (col_num[i],col_num[j],v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PCA** ---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.72148871,  0.13908504])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final output table\n",
    "dt_all_pca = pd.DataFrame()\n",
    "\n",
    "# highly correlated features\n",
    "high_corr = .7\n",
    "col_high_corr = set(col_num[i] for (v, i, j) in s_corr_list if v >= high_corr).union(set(col_num[j] for (v, i, j) in s_corr_list if v >= high_corr))\n",
    "np_high_corr = np.array(dt_all[list(col_high_corr)])\n",
    "\n",
    "# pca\n",
    "pca = PCA(n_components = 2)\n",
    "dt_all_pca = pd.DataFrame(pca.fit_transform(np_high_corr))\n",
    "\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simple Interaction** ---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# final output table\n",
    "dt_all_highCorr_sum, dt_all_highCorr_sub, dt_all_highCorr_times = simpleInter_num(dt_all[list(col_high_corr)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(313864, 28) (313864, 28) (313864, 28)\n"
     ]
    }
   ],
   "source": [
    "print(\"{} {} {}\".format(dt_all_highCorr_sum.shape, dt_all_highCorr_sub.shape, dt_all_highCorr_times.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3 prob of col_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# final output table\n",
    "dt_all_prob_cat = pd.DataFrame()\n",
    "\n",
    "for col in col_cat:\n",
    "    freq = dt_all[col].value_counts()\n",
    "    prob = freq / sum(freq)\n",
    "\n",
    "    dt_prob = pd.DataFrame({\"prob\": prob})\n",
    "    dt_prob = dt_prob.reset_index()\n",
    "\n",
    "    dt_all_prob_cat[col + \"_prob\"] = pd.merge(dt_all[col].reset_index(), dt_prob, left_on = col, right_on = \"index\")[\"prob\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.4 entropy (optional, maybe too similar to prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# final output table\n",
    "dt_all_entropy_cat = pd.DataFrame()\n",
    "\n",
    "for col in col_cat:\n",
    "    freq = dt_all[col].value_counts()\n",
    "    prob = freq / sum(freq)\n",
    "    entropy = - prob * np.log(prob)\n",
    "    \n",
    "    dt_entropy = pd.DataFrame({\"entropy\": prob})\n",
    "\n",
    "    dt_all_entropy_cat[col + \"_prob\"] = pd.merge(dt_all[col].reset_index(), dt_entropy.reset_index(), left_on = col, right_on = \"index\")[\"entropy\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.5 joint entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# final output table\n",
    "dt_train_jointEntropy_cat = pd.DataFrame()\n",
    "dt_test_jointEntropy_cat = pd.DataFrame()\n",
    "\n",
    "for col in col_cat:\n",
    "    # cat and grouped target and the combined\n",
    "    dt_groupedTarget_cat = pd.DataFrame({col: dt_train[col].values\n",
    "                                         , \"groupedTarget\": groupedTarget\n",
    "                                         , col + \"_groupedTarget\": dt_train[col].map(str) + pd.Series(groupedTarget).map(str)})\n",
    "\n",
    "    freq = dt_groupedTarget_cat[col + \"_groupedTarget\"].value_counts()\n",
    "    prob = freq / sum(freq)\n",
    "\n",
    "    jointEntropy =  - prob * np.log(prob)\n",
    "\n",
    "    dt_jointEntropy = pd.DataFrame({\"jointEntropy\": jointEntropy})\n",
    "\n",
    "    dt_jointEntropy[col] = dt_jointEntropy.reset_index()[\"index\"].str.slice(0, 1).values\n",
    "    dt_jointEntropy_sum = dt_jointEntropy.groupby(col)[\"jointEntropy\"].sum()\n",
    "\n",
    "    dt_jointEntropy_sum = pd.DataFrame({\"jointEntropy\": dt_jointEntropy_sum})\n",
    "    dt_train_jointEntropy_cat[col + \"_groupedTarget\"] = pd.merge(dt_groupedTarget_cat[col].reset_index(), dt_jointEntropy_sum.reset_index(), left_on = col, right_on = col)[\"jointEntropy\"]\n",
    "    dt_test_jointEntropy_cat[col + \"_groupedTarget\"] = pd.merge(dt_test[col].reset_index(), dt_jointEntropy_sum.reset_index(), left_on = col, right_on = col)[\"jointEntropy\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.6 target mean (maybe only apply to high cardinality feats; add random noise; leave on out?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# final output table\n",
    "dt_train_tm_cat = pd.DataFrame()\n",
    "dt_test_tm_cat = pd.DataFrame()\n",
    "\n",
    "for col in col_cat:\n",
    "    # cat + target\n",
    "    dt_cat_target = pd.DataFrame({col: dt_train[col].values\n",
    "                                 , \"target\": y_train})\n",
    "\n",
    "    # group col_cat\n",
    "    grouped = dt_cat_target.groupby(col)\n",
    "    # target mean for col_cat\n",
    "    dt_targetMean = pd.DataFrame({\"tm\": grouped[\"target\"].mean()})\n",
    "\n",
    "    # add target mean to final output table\n",
    "    dt_train_tm_cat[col + \"_tm\"] = pd.merge(dt_cat_target[col].reset_index(), dt_targetMean.reset_index(), left_on = col, right_on = col)[\"tm\"]\n",
    "\n",
    "    # add to test\n",
    "    dt_test_tm_cat[col + \"_tm\"] = pd.merge(dt_test[col].reset_index(), dt_targetMean.reset_index(), how = \"left\", left_on = col, right_on = col)[\"tm\"]\n",
    "\n",
    "    # avoid leakage (by doing kfold tm)\n",
    "    temp = np.zeros(nrow_train)\n",
    "\n",
    "    for i in np.arange(0, 4):\n",
    "        ids = np.arange(i, nrow_train, 4)\n",
    "\n",
    "        dt_temp = pd.DataFrame({\"tm\": dt_cat_target.drop(ids, axis = 0).groupby(col)[\"target\"].mean()})\n",
    "        temp[ids] = pd.merge(dt_cat_target.iloc[ids, :][col].reset_index(), dt_temp.reset_index(), how = \"left\", left_on = col, right_on = col)[\"tm\"]\n",
    "\n",
    "    dt_train_tm_cat[col + \"_tm\"] = temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.7 n-way interaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.8 simple stats"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:datasci]",
   "language": "python",
   "name": "conda-env-datasci-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
